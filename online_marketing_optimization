# Importing necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from scipy.stats import chi2_contingency
import rpy2.robjects as robjects
from rpy2.robjects import pandas2ri
pandas2ri.activate()

# Load data
def load_data(file_path):
    return pd.read_csv(file_path)

# Preprocess data
def preprocess_data(data):
    # Handle missing values
    data.fillna(method='ffill', inplace=True)

    # Feature Engineering
    # Eg: Calculate engagement score, conversion rates
    data['Engagement_Score'] = (data['Page_Views'] * data['Avg_Session_Duration']) / data['Bounce_Rate']
    data['Conversion_Rate'] = data['Leads_Generated'] / data['Total_Visitors']

    return data

# Exploratory Data Analysis
def perform_eda(data):
    # Pairplot for selected features
    sns.pairplot(data[['Total_Visitors', 'Leads_Generated', 'Engagement_Score', 'Conversion_Rate']])
    plt.show()

    # Correlation heatmap
    plt.figure(figsize=(10, 8))
    sns.heatmap(data.corr(), annot=True, cmap='coolwarm')
    plt.title("Correlation Matrix")
    plt.show()

# Advanced Data Analysis
def advanced_analysis(data):
    # K-Means Clustering for Visitor Segmentation
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data[['Engagement_Score', 'Conversion_Rate']])
    
    # Determine optimal number of clusters
    silhouette_scores = []
    for k in range(2, 11):
        kmeans = KMeans(n_clusters=k, random_state=42)
        kmeans.fit(scaled_data)
        score = silhouette_score(scaled_data, kmeans.labels_)
        silhouette_scores.append(score)

    optimal_k = silhouette_scores.index(max(silhouette_scores)) + 2
    kmeans = KMeans(n_clusters=optimal_k, random_state=42)
    clusters = kmeans.fit_predict(scaled_data)

    # Add cluster information to the dataset
    data['Cluster'] = clusters

    # Chi-Square Test for Categorical Data Analysis
    contingency_table = pd.crosstab(data['Traffic_Source'], data['Cluster'])
    chi2, p, dof, expected = chi2_contingency(contingency_table)
    print(f"Chi-Square Test: p-value = {p}")

    return data

# Integration with R for additional analysis
def r_integration(data):
    # Converting DataFrame to R object
    r_dataframe = pandas2ri.py2rpy(data)

    # Example R code - Replace with actual R analysis
    robjects.r('''
        library(ggplot2)
        r_function <- function(data) {
            p <- ggplot(data, aes(x=Engagement_Score, y=Conversion_Rate, color=factor(Cluster))) +
                geom_point() +
                labs(title="Cluster Analysis in R")
            print(p)
        }
    ''')

    r_func = robjects.globalenv['r_function']
    r_func(r_dataframe)

# Main function
def main():
    data = load_data('path/to/marketing_data.csv')
    processed_data = preprocess_data(data)
    perform_eda(processed_data)
    analyzed_data = advanced_analysis(processed_data)
    r_integration(analyzed_data)

if __name__ == "__main__":
    main()
